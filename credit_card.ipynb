{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Card Fraud Detection using Isolation Forest\n\nThis notebook has been prepared as part of the HSLU Fraud Detection Module in the MSc of Applied Information and Data Science.\n\n##Â What is Isolation Forest?\n\nIsolation Forest (Liu et al., 2008) is an anomaly detection method. There exist many more, but the algorithm is quite interesting in the way it works. Since anomalous data usually is sparse, anomaly detection algorithms oftentimes work by learning on clean data (learning the normal cases) and then is able to classify anomalous data due to it's *inability* to fit the anomalous data. Other candidates are for instance support vector machines, neural networks, etc.\n\nIsolation Forest (IF) is based on decision trees. The general idea behind IF is that the algorithm recursively selects a feature of the data and then partitions the data by selecting a random value within said feature. By doing so, it tries to isolate each data point. It is intuitive that anomalous data points require fewer recursive steps than normal data points to be fully described.\n\nAs simple as the algorithm, it is striking yet how powerful it is. Despite it being introduced in 2008, it still has applications in todays ML world. There is a large amount of papers published in recent years<sup>1</sup>.\n\n<sup>1</sup> Google Scholar shows 42k results since 2017 for Isolation Forest.","metadata":{}},{"cell_type":"markdown","source":"## Setup\nFirst, let's load the data and have a look.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join('..', 'input', 'creditcardfraud', 'creditcard.csv'))\nprint(df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a large dataset (284807 rows, 31 columns - incl. target). It contains a time offset column, an amount column and 28 encoded value columns. As we can see in the following plot (and output), the dataset is highly imbalanced. If we want to build a really powerful model, we would need to seriously consider the imbalance and apply appropriate techniques (such as SMOTE).","metadata":{}},{"cell_type":"code","source":"print(df.Class.value_counts())\ndf.Class.value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nFor the training, we will use a subset of the \"non-fraudulent\" class to train Isolation Forest. For the validation we will construct a set of data using both classes (equal split).","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's first construct our datasets.\n\nIn an initial attempt, we will only train the isolation forest using a clean (normal) dataset.","metadata":{}},{"cell_type":"code","source":"seed = 1337\n\ndef get_data(df, clean_train=True):\n    \"\"\"\n        clean_train=True returns a train sample that only contains clean samples.\n        Otherwise, it will return a subset of each class in train and test (10% outlier)\n    \"\"\"\n    clean = df[df.Class == 0].copy().reset_index(drop=True)\n    fraud = df[df.Class == 1].copy().reset_index(drop=True)\n    print(f'Clean Samples: {len(clean)}, Fraud Samples: {len(fraud)}')\n\n    if clean_train:\n        train, test_clean = train_test_split(clean, test_size=len(fraud), random_state=seed)\n        print(f'Train Samples: {len(train)}')\n\n        test = pd.concat([test_clean, fraud]).reset_index(drop=True)\n\n        print(f'Test Samples: {len(test)}')\n\n        # shuffle the test data\n        test.sample(frac=1, random_state=seed).reset_index(drop=True)\n        \n        train_X, train_y = train.loc[:, ~train.columns.isin(['Class'])], train.loc[:, train.columns.isin(['Class'])]\n        test_X, test_y = test.loc[:, ~test.columns.isin(['Class'])], test.loc[:, test.columns.isin(['Class'])]\n    else:\n        clean_train, clean_test = train_test_split(clean, test_size=int(len(fraud)+(len(fraud)*0.9)), random_state=seed)\n        fraud_train, fraud_test = train_test_split(fraud, test_size=int(len(fraud)*0.1), random_state=seed)\n        \n        train_samples = pd.concat([clean_train, fraud_train]).reset_index(drop=True)\n        test_samples = pd.concat([clean_test, fraud_test]).reset_index(drop=True)\n        \n        # shuffle\n        train_samples.sample(frac=1, random_state=seed).reset_index(drop=True)\n        \n        print(f'Train Samples: {len(train_samples)}')\n        test_samples.sample(frac=1, random_state=seed).reset_index(drop=True)\n        \n        print(f'Test Samples: {len(test_samples)}')\n        train_X, train_y = train_samples.loc[:, ~train_samples.columns.isin(['Class'])], train_samples.loc[:, train_samples.columns.isin(['Class'])]\n        test_X, test_y = test_samples.loc[:, ~test_samples.columns.isin(['Class'])], test_samples.loc[:, test_samples.columns.isin(['Class'])]\n    \n    return train_X, train_y, test_X, test_y\n\ntrain_X, train_y, test_X, test_y = get_data(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's train our model.","metadata":{}},{"cell_type":"code","source":"model = IsolationForest(random_state=seed)\nmodel.fit(train_X, train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how we do.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(X):\n    test_yhat = model.predict(X)\n    # values are -1 and 1 (-1 for outliers and 1 for inliers), thus we will map it to 0 (inlier) and 1 (outlier) as this is our target variable\n    test_yhat = np.array([1 if y == -1 else 0 for y in test_yhat])\n    return test_yhat\n\ntest_yhat = predict(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_confusion_matrix(test_y, test_yhat):\n    cm = confusion_matrix(test_y, test_yhat)\n\n    fig, ax = plt.subplots(1, 1,figsize=(10,10))\n\n    tick_labels = ['Legitimate','Fraudulent']\n\n    # calculate output\n    total = np.sum(cm)\n    tp = cm[0][0]\n    fn = cm[0][1]\n    fp = cm[1][0]\n    tn = cm[1][1]\n    tp_o = np.round((tp / total)*100,3)\n    fn_o = np.round((fn / total)*100,3)\n    fp_o = np.round((fp / total)*100,3)\n    tn_o = np.round((tn / total)*100,3)\n    tp_c = np.round(max((tp / (tp + fn)), 0.0)*100,3)\n    fn_c = np.round(max((fn / (tp + fn)), 0.0)*100,3)\n    fp_c = np.round(max((fp / (fp + tn)), 0.0)*100,3)\n    tn_c = np.round(max((tn / (fp + tn)), 0.0)*100,3)\n    labels = np.array([[\n            f'TP\\nCount: {tp}\\nOverall: {tp_o}%\\nTPR: {tp_c}%', \n            f'FN (Type 2)\\nCount: {fn}\\nOverall: {fn_o}%\\nFNR: {fn_c}%'\n        ],[\n            f'FP (Type 1)\\nCount: {fp}\\nOverall: {fp_o}%\\nFPR: {fp_c}%', \n            f'TN\\nCount: {tn}\\nOverall: {tn_o}%\\nTNR: {tn_c}%'\n        ]])\n    sns.heatmap(cm, ax=ax, annot=labels, fmt='', cmap=plt.cm.RdYlBu)\n\n    ax.set_title('Confusion Matrix', fontsize=14)\n\n    ax.set_xticklabels(tick_labels, fontsize=14, rotation=0)\n    ax.set_xlabel('Predicted', fontsize=20)\n    ax.set_yticklabels(tick_labels, fontsize=14, rotation=0)\n    ax.set_ylabel('Actual', fontsize=20)\n    plt.show()\n\nget_confusion_matrix(test_y, test_yhat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_classification_report(test_y, test_yhat):\n    labels = ['Legitimate','Fraudulent']\n    print(classification_report(test_y, test_yhat, target_names=labels))\nget_classification_report(test_y, test_yhat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the classification report, we achieve an f1-score of 89% which is really good in an imbalanced data set.\nLet's see how our model does if not only train on clean samples. Does it make a difference whether we train it with mixed clean + anomalous samples or not?","metadata":{}},{"cell_type":"code","source":"train_X, train_y, test_X, test_y = get_data(df, clean_train=False)\nmodel = IsolationForest(random_state=seed)\nmodel.fit(train_X, train_y)\ntest_yhat = predict(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_confusion_matrix(test_y, test_yhat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_classification_report(test_y, test_yhat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears to be that this time around we perform a lot worse on the fraudulent cases (f1-score of 0.67). Overall, we are pretty good since we are now a lot more confident when predicting legitimate cases. However, since we want to be good in predicting fraudulent cases, this is not really acceptable.","metadata":{}},{"cell_type":"markdown","source":"# References\n\nLiu, F. T., Ting, K. M., & Zhou, Z. H. (2008, December). Isolation forest. In 2008 eighth ieee international conference on data mining (pp. 413-422). IEEE.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}